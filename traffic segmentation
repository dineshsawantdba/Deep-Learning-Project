# -*- coding: utf-8 -*-
"""Segmentation_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GW7121Z_0RuKL3MVjUliI_htL8W5NRON

# Segmentation of Indian Traffic
"""

!pip install tensorflow==2.2.0

!pip install keras==2.3.1

!pip install -U segmentation-models==0.2.1

import math
from PIL import Image, ImageDraw
from PIL import ImagePath
import pandas as pd
import os
from os import path
from tqdm import tqdm
import json
import cv2
import numpy as np
import matplotlib.pyplot as plt
import urllib
from collections import Counter

import tensorflow as tf
tf.test.gpu_device_name()
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)



# we are importing the pretrained unet from the segmentation models
# https://github.com/qubvel/segmentation_models
import segmentation_models as sm
from segmentation_models import Unet
# sm.set_framework('tf.keras')
tf.keras.backend.set_image_data_format('channels_last')

# loading the unet model and using the resnet 34 and initilized weights with imagenet weights
# "classes" :different types of classes in the dataset
model = Unet('resnet34', encoder_weights='imagenet', classes=1, activation='sigmoid', input_shape=(512,512,3))



!wget --header="Host: doc-08-c8-docs.googleusercontent.com" --header="User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36" --header="Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9" --header="Accept-Language: en-US,en;q=0.9,mr;q=0.8,es;q=0.7" --header="Referer: https://drive.google.com/drive/u/0/my-drive" --header="Cookie: AUTH_dgh1oc5ukk6marmo39rrgovnupvviul7_nonce=rvdh2jlmdqiaa" --header="Connection: keep-alive" "https://doc-08-c8-docs.googleusercontent.com/docs/securesc/54d8lnisii2tkf625lh1pal1ag2fevvn/cr6c2hlsrnh8u2qeopnootoo5nho5gsd/1599109650000/17085903079421739912/17085903079421739912/1gF0yGLsKEh7jYbS99bbubJWB2i30QL2Z?e=download&authuser=0&nonce=rvdh2jlmdqiaa&user=17085903079421739912&hash=qoqgmpb1blgj3h157kud5ffkl0rr2oj5" -c -O 'Copy of data.zip'

import zipfile
with zipfile.ZipFile("Copy of data.zip","r") as zip_ref:
    zip_ref.extractall("/content/")



"""<pre>
1. You can download the data from this link, and extract it

2. All your data will be in the folder "data" 

3. Inside the data you will be having two folders

|--- data
|-----| ---- images
|-----| ------|----- Scene 1
|-----| ------|--------| ----- Frame 1 (image 1)
|-----| ------|--------| ----- Frame 2 (image 2)
|-----| ------|--------| ----- ...
|-----| ------|----- Scene 2
|-----| ------|--------| ----- Frame 1 (image 1)
|-----| ------|--------| ----- Frame 2 (image 2)
|-----| ------|--------| ----- ...
|-----| ------|----- .....
|-----| ---- masks
|-----| ------|----- Scene 1
|-----| ------|--------| ----- json 1 (labeled objects in image 1)
|-----| ------|--------| ----- json 2 (labeled objects in image 1)
|-----| ------|--------| ----- ...
|-----| ------|----- Scene 2
|-----| ------|--------| ----- json 1 (labeled objects in image 1)
|-----| ------|--------| ----- json 2 (labeled objects in image 1)
|-----| ------|--------| ----- ...
|-----| ------|----- .....
</pre>

# Task 1: Preprocessing

## 1. Get all the file name and corresponding json files
"""

def return_file_names_df(root_dir):
  """
  This function Returns Pandas Dataframe,
  argument parameter: Root Directory
  this function takes root directory as argument, 
  there are 2 folders in data images  and mask file , 
  the path of images taken and saved in the list 
  that list is converted in to column Image of Pandas Dataframe.
  the path of jason file  taken and saved in the list 
  that list is converted in to column Jason of Pandas Dataframe
  """
  # write the code that will create a dataframe with two columns ['images', 'json']
    # the column 'image' will have path to images
    # the column 'json' will have path to json files
  

  image_folder=[]
  images_name=[]

  for folder in os.listdir('data/images'):
    image_folder.append(int(folder)) 
    #print(image_folder)

  image_folder.sort()
  for i in range(len(image_folder)):
    data_path_for_image_folder= 'data/images/' + str(image_folder[i]) + '/'
    #print(data_path_for_image_folder)
    for img_filename in sorted(os.listdir(data_path_for_image_folder)):
      #print(img_filename)
      if img_filename.endswith(".jpg"):
          images_name.append(data_path_for_image_folder+img_filename)

 
  mask_folder=[]
  mask_name=[]

  for folder in os.listdir('data/mask'):
    mask_folder.append(int(folder)) 
    #print(image_folder)

  mask_folder.sort()

  for i in range(len(mask_folder)):
    data_path_for_mask_folder= 'data/mask/'+ str(mask_folder[i]) + '/'
    #print(data_path_for_image_folder)
    for mask_filename in sorted(os.listdir(data_path_for_mask_folder)):
      #print(img_filename)
      if mask_filename.endswith(".json"):
          mask_name.append(data_path_for_mask_folder+mask_filename)

  dict_image_jason={'images':images_name,'json':mask_name}
  data_df=pd.DataFrame.from_dict(dict_image_jason)

  return data_df

import pandas as pd
data_df = return_file_names_df('/content/data')
data_df.head()

"""> If you observe the dataframe, we can consider each row as single data point, where first feature is image and the second feature is corresponding json file"""

def grader_1(data_df):
    for i in data_df.values:
        if not (path.isfile(i[0]) and path.isfile(i[1]) and i[0][12:i[0].find('_')]==i[1][10:i[1].find('_')]):
            return False
    return True

grader_1(data_df)

data_df.shape

"""## 2. Structure of sample Json file

<img src='https://i.imgur.com/EfR5KmI.png' width="200" height="100">

* Each File will have 3 attributes
    * imgHeight: which tells the height of the image
    * imgWidth: which tells the width of the image
    * objects: it is a list of objects, each object will have multiple attributes,
        * label: the type of the object
        * polygon: a list of two element lists, representing the coordinates of the polygon

#### Compute the unique labels

Let's see how many unique objects are there in the json file.
to see how to get the object from the json file please check <a href='https://www.geeksforgeeks.org/read-json-file-using-python/'>this blog </a>
"""

def return_unique_labels(data_df):
    labelsss=[]
    label_all=[]
    unique_labels=[]
    labels_counts=[]
    # for each file in the column json
    #read and store all the objects present in that file
    # compute the unique objects and retrun them
    # if open any json file using any editor you will get better sense of it
    for j in tqdm(range(data_df.shape[0])): 
      mask_path=data_df['json'][j]
      f=open(mask_path)
      data = json.load(f)
      #dict_data=data['objects']
  
      for i in data['objects']:
        labelsss.append(i['label'])
        if i['label'] not in unique_labels:
          unique_labels.append(i['label'])

        #print(data['objects'][i]['label'])

      label_set=set(labelsss)
      label_list=list(label_set)
      label_all.append(label_list)
  
 
    for i in range(len(label_all)):
       for j in range(len(label_all[i])):
         labels_counts.append(label_all[i][j])

       #print(labels_counts)
        #print(Counter(labels_counts))
    #label_clr=dict(Counter(labels_counts))

    return unique_labels

unique_labels = return_unique_labels(data_df)

"""<img src='https://i.imgur.com/L4QH6Tp.png'>"""

label_clr = {'road':10, 'parking':20, 'drivable fallback':20,'sidewalk':30,'non-drivable fallback':40,'rail track':40,\
                        'person':50, 'animal':50, 'rider':60, 'motorcycle':70, 'bicycle':70, 'autorickshaw':80,\
                        'car':80, 'truck':90, 'bus':90, 'vehicle fallback':90, 'trailer':90, 'caravan':90,\
                        'curb':100, 'wall':100, 'fence':110,'guard rail':110, 'billboard':120,'traffic sign':120,\
                        'traffic light':120, 'pole':130, 'polegroup':130, 'obs-str-bar-fallback':130,'building':140,\
                        'bridge':140,'tunnel':140, 'vegetation':150, 'sky':160, 'fallback background':160,'unlabeled':0,\
                        'out of roi':0, 'ego vehicle':170, 'ground':180,'rectification border':190,\
                   'train':200}

def grader_2(unique_labels):
    if (not (set(label_clr.keys())-set(unique_labels))) and len(unique_labels) == 40:
        print("True")
    else:
        print("Flase")

grader_2(unique_labels)

"""<pre>
* here we have given a number for each of object types, if you see we are having 21 different set of objects
* Note that we have multiplies each object's number with 10, that is just to make different objects look differently in the segmentation map
* Before you pass it to the models, you might need to devide the image array /10.
</pre>

## 3. Extracting the polygons from the json files
"""

def get_poly(file):

  # this function will take a file name as argument
    
    # it will process all the objects in that file and returns
    
    # label: a list of labels for all the objects label[i] will have the corresponding vertices in vertexlist[i]
    # len(label) == number of objects in the image
    
    # vertexlist: it should be list of list of vertices in tuple formate 
    # ex: [[(x11,y11), (x12,y12), (x13,y13) .. (x1n,y1n)]
    #     [(x21,y21), (x22,y12), (x23,y23) .. (x2n,y2n)]
    #      .....
    #     [(xm1,ym1), (xm2,ym2), (xm3,ym3) .. (xmn,ymn)]]
    # len(vertexlist) == number of objects in the image
    
    # * note that label[i] and vertextlist[i] are corresponds to the same object, one represents the type of the object
    # the other represents the location
    
    # width of the image
    # height of the image
  mask_path=file
  f=open(mask_path)
  data = json.load(f)
  h=data['imgHeight']
  w=data['imgWidth']
  len(data['objects'])
  label=[]
  for i in data['objects']:
    label.append(i['label'])

  obej=data['objects']
  #print(obej[1])
  #print(len(obej[0]['polygon']))
  vertexlist=[]
  for j in range(len(obej)):
    list=[]
    for k in range(len(obej[j]['polygon'])):
      #print(tuple(obej[0]['polygon'][k]))
      list.append(tuple(obej[j]['polygon'][k]))
    vertexlist.append(list)
  return w, h, label, vertexlist

def grader_3(file):
    w, h, labels, vertexlist = get_poly(file)
    print(len((set(labels)))==18 and len(vertexlist)==227 and w==1920 and h==1080 \
          and isinstance(vertexlist,list) and isinstance(vertexlist[0],list) and isinstance(vertexlist[0][0],tuple) )

grader_3('data/mask/201/frame0029_gtFine_polygons.json')

"""## 4. Creating Image segmentations by drawing set of polygons

### Example
"""

import math 
from PIL import Image, ImageDraw 
from PIL import ImagePath  
side=8
x1 = [ ((math.cos(th) + 1) *9, (math.sin(th) + 1) * 6) for th in [i * (2 * math.pi) / side for i in range(side)] ]
x2 = [ ((math.cos(th) + 2) *9, (math.sin(th) + 3) *6) for th in [i * (2 * math.pi) / side for i in range(side)] ]

img = Image.new("RGB", (28,28))
img1 = ImageDraw.Draw(img)
# please play with the fill value
# writing the first polygon
img1.polygon(x1, fill =20)
# writing the second polygon
img1.polygon(x2, fill =30)

img=np.array(img)
# note that the filling of the values happens at the channel 1, so we are considering only the first channel here
plt.imshow(img[:,:,0])
print(img.shape)
print(img[:,:,0]//10)
im = Image.fromarray(img[:,:,0])
im.save("test_image.png")

#plt.plot(vertexlist[0])

#label_clr

classes = list(np.arange(1,22,1))
print(classes)

LIST = ['CLASS_'+str(classes[i]) for i in range(len(classes))]

values = list(np.arange(10,220,10))

print(values)

res = {LIST[i] : values[i] for i in range(len(LIST))}

print(res.keys())

classes_list = list(res.keys())
print(classes_list)

import PIL.ImageDraw as ImageDraw
import PIL.Image as Image

image = Image.new("RGB", (640, 480))

draw = ImageDraw.Draw(image)

# points = ((1,1), (2,1), (2,2), (1,2), (0.5,1.5))
points = ((100, 100), (200, 100), (200, 200), (100, 200), (50, 150))
draw.polygon((points), fill=200)

image.show()

pip install pillow

def compute_masks(data_df):
    # after you have computed the vertexlist plot that polygone in image like this
    
    # img = Image.new("RGB", (w, h))
    # img1 = ImageDraw.Draw(img)
    # img1.polygon(vertexlist[i], fill = label_clr[label[i]])
    
    # after drawing all the polygons that we collected from json file, 
    # you need to store that image in the folder like this "data/output/scene/framenumber_gtFine_polygons.png"
    
    # after saving the image into disk, store the path in a list
    # after storing all the paths, add a column to the data_df['mask'] ex: data_df['mask']= mask_paths
    
    return data_df

w ,h ,labels,vertexlist = get_poly(data_df.json[1])
img = Image.new("RGB", (w, h))
img1 = ImageDraw.Draw(img)
for i in range(len(vertexlist)):
  img1.polygon(vertexlist[i], fill = label_clr[labels[i]])
img = np.array(img)
plt.imshow(img[:,:,0])
print(img.shape)
print(img[:,:,0]//10)
im = Image.fromarray(img[:,:,0])
im = im.save("test_image.png")

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
image = mpimg.imread(data_df.images[1])
plt.imshow(image)
plt.show()





path = data_df.json[2]
sep = '.'
mask_name = path.split(sep)[0]
rest = mask_name
#print(rest)
sep = '/'
mask_name = path.split(sep)
#print(mask_name)
png_mask_path = rest+'.png'
#print(png_mask_path)
path = data_df.json[2]
sep = '/'
output_folder = path.split(sep)
output_folder_path = output_folder[0]+'/output/'+output_folder[2]
#print(output_folder_path)
#print(mask_name)
abs_mask_name = mask_name[3]
sep = '.'
abs_mask_name = abs_mask_name.split(sep)[0]
#print(abs_mask_name)
abs_mask_path = output_folder_path+'/'+abs_mask_name+'.png'
print(abs_mask_path)

def compute_masks(data_df):


  mask_name_list = []

  for k in tqdm(range(data_df.shape[0])):
    w ,h ,labels,vertexlist = get_poly(data_df.json[k])
    img = Image.new("RGB", (w, h))
    img1 = ImageDraw.Draw(img)
    for i in range(len(vertexlist)):
      if len(vertexlist[i])==0:
        img1.polygon([(0,0),(0,0)],fill=0)
      elif len(vertexlist[i])==1:
        img1.polygon([(0,0),(0,0)],fill=0)
      else:
        img1.polygon(vertexlist[i], fill = label_clr[labels[i]])
    
    img = np.array(img)
    im = Image.fromarray(img[:,:,0])
  
    path = data_df.json[k]
    sep = '.'
    mask_name = path.split(sep)[0]
    rest = mask_name
    #print(rest)
    sep = '/'
    mask_name = path.split(sep)
    #print(mask_name)
    png_mask_path = rest+'.png'
    #print(png_mask_path)
    path = data_df.json[k]
    sep = '/'
    output_folder = path.split(sep)
    output_folder_path = output_folder[0]+'/output/'+output_folder[2]
    #print(output_folder_path)
    #print(mask_name)
    abs_mask_name = mask_name[3]
    sep = '.'
    abs_mask_name = abs_mask_name.split(sep)[0]
    #print(abs_mask_name)
    #abs_mask_path = output_folder_path+'/'+abs_mask_name+'.png'
    #print(abs_mask_path)
  
    if not os.path.exists(output_folder_path):
      os.makedirs(output_folder_path)

    abs_mask_path = output_folder_path+'/'+abs_mask_name+'.png'
    im.save(abs_mask_path)
    mask_name_list.append(abs_mask_path)

  data_df['mask'] = mask_name_list
  return data_df

data_df.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# data_df = compute_masks(data_df)

data_df.shape

my_img = cv2.imread('data/output/201/frame0029_gtFine_polygons.png')    
plt.imshow(my_img)



import urllib.request

data_df = compute_masks(data_df)
data_df.head()

def grader_3():
    url = "https://i.imgur.com/4XSUlHk.png"
    url_response = urllib.request.urlopen(url)
    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)
    img = cv2.imdecode(img_array, -1)
    my_img = cv2.imread('data/output/201/frame0029_gtFine_polygons.png')    
    plt.imshow(my_img)
    print((my_img[:,:,0]==img).all())
    print(np.unique(img))
    print(np.unique(my_img[:,:,0]))
    data_df.to_csv('preprocessed_data.csv', index=False)
grader_3()

"""# Task 2: Applying Unet to segment the images

<pre>
* please check the paper: https://arxiv.org/abs/1505.04597

* <img src='https://i.imgur.com/rD4yP7J.jpg' width="500">

* As a part of this assignment we won't writingt this whole architecture, rather we will be doing transfer learning

* please check the library <a hreaf='https://github.com/qubvel/segmentation_models'>https://github.com/qubvel/segmentation_models</a>

* You can install it like this "pip install -U segmentation-models==0.2.1", even in google colab you can install the    same with "!pip install -U segmentation-models==0.2.1" 

* Check the reference notebook in which we have solved one end to end case study of image forgery detection using same  unet

* The number of channels in the output will depend on the number of classes in your data, since we know that we are having 21 classes, the number of channels in the output will also be 21

* <strong>This is where we want you to explore, how do you featurize your created segmentation map note that the original map will be of (w, h, 1) and the output will be (w, h, 21) how will you calculate the loss</strong>, you can check the examples in segmentation github

* please use the loss function that is used in the refence notebooks

</pre>
"""

import tensorflow as tf
# tf.enable_eager_execution()
import os
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
# from hilbert import hilbertCurve
import imgaug.augmenters as iaa
import numpy as np
# import albumentations as A
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
import numpy as np
np.random.seed(42)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
import tensorflow as tf
tf.random.set_seed(42)

from tensorflow.keras import layers
from tensorflow.keras.layers import Dense,Input,Conv2D,MaxPool2D,Activation,Dropout,Flatten, BatchNormalization, ReLU, Reshape
from tensorflow.keras.models import Model
import random as rn
#from tensorflow.keras.layers import CuDNNLSTM
from tensorflow.keras.layers import Flatten

import tensorflow
print(tensorflow.__version__)

data_df = pd.read_csv('/content/preprocessed_data.csv')

data_df.head()

from sklearn.model_selection import train_test_split
X_train, X_test = train_test_split(data_df,test_size=0.2,random_state=42)

X_train.shape

image = cv2.imread(data_df.images[0])
print(image.shape)

#!pip install -U segmentation-models==0.2.1

#Import imgaug.augmenters as iaa
# For the assignment choose any 4 augumentation techniques
# check the imgaug documentations for more augmentations

aug2 = iaa.Fliplr(1)
aug3 = iaa.Flipud(1)
aug4 = iaa.Emboss(alpha = (1) , strength=1)
aug5 = iaa.DirectedEdgeDetect(alpha=(0.8) , direction=(1.0))
aug6 = iaa.Sharpen(alpha=(1.0), lightness=(1.5))

def visualize(**images) :
  n = len(images)
  plt.figure(figsize=(16,5))
  for i , (name,image) in enumerate(images.items()):
    plt.subplot(1, n, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.title(' '.join(name.split('_')).title())
    if i == 1:
      plt.imshow(image,cmap=gray,vmax=1,vmin=0)
    else :
      plt.imshow(image)
  plt.show()

def normalize_image(mask):
  mask=mask/255
  return mask

class Dataset:
  # we will be modifying this CLASSES according to your data/problems
  CLASSES = ['CLASS_1', 'CLASS_2', 'CLASS_3', 'CLASS_4', 'CLASS_5', 'CLASS_6', 
             'CLASS_7', 'CLASS_8', 'CLASS_9', 'CLASS_10', 'CLASS_11', 'CLASS_12', 'CLASS_13',
             'CLASS_14', 'CLASS_15', 'CLASS_16', 'CLASS_17', 'CLASS_18', 'CLASS_19', 
             'CLASS_20', 'CLASS_21']
  # the parameters needs to changed based on your requirements
  # here we are collecting the file_names because in our dataset, both our images and maks will have same file name
  # ex: fil_name.jpg   file_name.mask.jpg

  def __init__(self,image_names,mask_names,classes):
    self.ids_image = image_names
    self.ids_mask = mask_names
    # The paths of images
    self.images_fps = [os.path.join(image_id) for image_id in self.ids_image]
    #The path of segmentation images
    self.mask_fps = [os.path.join(mask_id) for mask_id in self.ids_mask]
    #Giving labels for each class
    self.class_values = [10, 20, 30, 40, 50, 60, 70, 80, 90,
                             100, 110, 120, 130, 140, 150, 160, 170,
                             180, 190, 200, 255]

  def __getitem__(self ,i):
    #read data
    image = cv2.imread(self.images_fps[i])
    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
    mask = cv2.imread(self.mask_fps[i],0)
    image = np.float32(cv2.resize(image,(512,512)))
    mask = np.float32(cv2.resize(mask,(512,512)))

    image_masks = [(mask == v) for v in self.class_values]
    mask = np.stack(image_masks,axis= -1).astype('float')

    a = np.random.uniform()
    if a < 0.2 :
      image = aug2.augment_image(image)
      image_mask = aug2.augment_image(mask)
    elif a < 0.4 :
      image = aug3.augment_image(image)
      image_mask = aug3.augment_image(mask)
    elif a < 0.6 :
      image = aug4.augment_image(image)
      image_mask = aug4.augment_image(mask)
    else :
      image = aug6.augment_image(image)
      image_mask = aug6.augment_image(mask)

    return image,image_mask

  def __len__(self):
    return len(self.ids_image)

class Dataloader(tf.keras.utils.Sequence):
  def __init__(self,dataset,batch_size=1,shuffle=False) :
    self.dataset = dataset
    self.batch_size = batch_size
    self.shuffle = shuffle
    self.indexes = np.arange(len(dataset))

  def __getitem__(self,i):
    start = i * self.batch_size
    stop = (i + 1) * self.batch_size
    data = []
    for j in range(start,stop):
      data.append(self.dataset[j])

    batch = [np.stack(samples,axis=0) for samples in zip(*data)]

    return tuple(batch)

  def __len__(self):
    return len(self.indexes) //self.batch_size

  def __on_epoch_end__(self):
    if self.shuffle:
      self.indexes = np.random.permutation(self.indexes)

train_images_names = list(X_train.images)
train_mask_names = list(X_train['mask'])

test_images_names = list(X_test.images)
test_mask_names = list(X_test['mask'])

print(classes_list)

classes_list = [x.lower() for x in classes_list]

print(classes_list)

classes_list=['class_1', 'class_2', 'class_3', 'class_4', 'class_5', 'class_6',
              'class_7', 'class_8', 'class_9', 'class_10', 'class_11', 'class_12',
              'class_13', 'class_14', 'class_15', 'class_16', 'class_17', 'class_18',
              'class_19', 'class_20', 'class_21']

train_dataset = Dataset(train_images_names,train_mask_names,classes=classes_list)
test_dataset = Dataset(test_images_names,test_mask_names,classes=classes_list)

train_dataloader = Dataloader(train_dataset,batch_size=8,shuffle=True)
test_dataloader = Dataloader(test_dataset, batch_size=8, shuffle=True)

print(train_dataloader[0][0].shape)
print(test_dataloader[0][1].shape)
assert train_dataloader[0][0].shape == ((8, 512, 512, 3))
assert train_dataloader[0][1].shape == ((8, 512, 512, 21))

# loading the unet model and using the resnet 34 and initilized weights with imagenet weights
# "classes" :different types of classes in the dataset
model = Unet('resnet34',
             encoder_weights='imagenet',
             classes=21,
             activation='sigmoid', 
             input_shape=(512,512,3))

model.summary()

# https://github.com/qubvel/segmentation_models
import segmentation_models as sm
from segmentation_models.metrics import iou_score
from segmentation_models import Unet
from keras.optimizers import Adam

optim = Adam(learning_rate = 0.0001)

focal_loss = sm.losses.cce_dice_loss

model.compile(optim, focal_loss,metrics=[iou_score])

from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau

callbacks = [
             ModelCheckpoint('./best_model.h5',save_weights_only=True,save_best_only=True,\
                             mode='min',monitor='val_iou_score'),
             ReduceLROnPlateau(monitor='val_iou_score',min_lr=0.000001,patience=2)
]

history = model.fit_generator(train_dataloader,steps_per_epoch=len(train_dataloader),epochs=200,\
                              validation_data=test_dataloader,callbacks=callbacks)





"""### Task 2.1: Dice loss

<pre>
* Explain the Dice loss
* 1. Write the formualtion
* 2. Range of the loss function
* 3. Interpretation of loss function
* 4. Write your understanding of the loss function, how does it helps in segmentation
</pre>

### Task 2.2: Training Unet

<pre>
* Split the data into 80:20.
* Train the UNET on the given dataset and plot the train and validation loss.
* As shown in the reference notebook plot 20 images from the test data along with its segmentation map, predicted map.
</pre>

# Task 3: Training CANet
"""

import tensorflow as tf
# tf.compat.v1.enable_eager_execution()
from tensorflow import keras
from tensorflow.keras.layers import *
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import UpSampling2D
from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D
from tensorflow.keras.layers import concatenate
from tensorflow.keras.layers import Multiply
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.initializers import glorot_uniform
K.set_image_data_format('channels_last')
K.set_learning_phase(1)

"""* as a part of this assignment we will be implementing the architecture based on this paper https://arxiv.org/pdf/2002.12041.pdf
* We will be using the custom layers concept that we used in seq-seq assignment
* You can devide the whole architecture can be devided into two parts
    1. Encoder
    2. Decoder
    <img src='https://i.imgur.com/prH3Mno.png' width="600">
* Encoder:
    * The first step of the encoder is to create the channel maps [$C_1$, $C_2$, $C_3$, $C_4$]
    * $C_1$ width and heigths are 4x times less than the original image
    * $C_2$ width and heigths are 8x times less than the original image
    * $C_3$ width and heigths are 8x times less than the original image
    * $C_4$ width and heigths are 8x times less than the original image
    * <i>you can reduce the dimensions by using stride parameter</i>.
    * [$C_1$, $C_2$, $C_3$, $C_4$] are formed by applying a "conv block" followed by $k$ number of "identity block". i.e the $C_k$ feature map will single "conv block" followed by $k$ number of "identity blocks".
    <table>
    <tr><td><img src="https://i.imgur.com/R8Gdypo.png" width="300"></td>
        <td><img src="https://i.imgur.com/KNunjQK.png" width="250"></td></tr>
    </table>
    * <strong>The conv block and identity block of $C_1$</strong>: the number filters in the covolutional layers will be $[4,4,8]$ and the number of filters in the parallel conv layer will also be $8$.
    * <strong>The conv block and identity block of $C_2$</strong>: the number filters in the covolutional layers will be $[8,8,16]$ and the number of filters in the parallel conv layer will also be $16$.
    * <strong>The conv block and identity block of $C_3$</strong>: the number filters in the covolutional layers will be $[16,16,32]$ and the number of filters in the parallel conv layer will also be $32$.
    * <strong>The conv block and identity block of $C_4$</strong>: the number filters in the covolutional layers will be $[32,32,64]$ and the number of filters in the parallel conv layer will also be $64$.
    * Here $\oplus$ represents the elementwise sum
    <br>
    
    <font color="red">NOTE: these filters are of your choice, you can explore more options also</font>
    
    * Example: if your image is of size $(512, 512, 3)$
        * the output after $C_1$ will be $128*128*8$
        * the output after $C_2$ will be $64*64*16$
        * the output after $C_3$ will be $64*64*32$
        * the output after $C_4$ will be $64*64*64$
"""

class convolutional_block(tf.keras.layers.Layer):
    def __init__(self, kernel=3,  filters=[4,4,8], stride=1, name="conv block"):
        super().__init__(name=name)
        self.F1, self.F2, self.F3 = filters
        self.kernel = kernel
        self.stride = stride
    def call(self, X):
        # write the architecutre that was mentioned above
        return X

class identity_block(tf.keras.layers.Layer):
    def __init__(self, kernel=3,  filters=[4,4,8], name="identity block"):
        super().__init__(name=name)
        self.F1, self.F2, self.F3 = filters
        self.kernel = kernel
    def call(self, X):
        # write the architecutre that was mentioned above
        return X

"""* The output of the $C_4$ will be passed to $\text{Chained Context Aggregation Module (CAM)}$
<img src='https://i.imgur.com/Bu63AAA.png' width="400">
* The CAM module will have two operations names Context flow and Global flow
* <strong>The Global flow</strong>: 
    * as shown in the above figure first we willl apply  <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D">global avg pooling</a> which results in (#, 1, 1, number_of_filters) then applying <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly">BN</a>, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU">RELU</a>, $1*1 \text{ Conv}$ layer sequentially which results a matrix (#, 1, 1, number_of_filters). Finally apply <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> / <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)
    * If you use <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> then use bilinear pooling as interpolation technique
* <strong>The Context flow</strong>: 
    * as shown in the above figure (c) the context flow will get inputs from two modules `a. C4` `b. From the above flow` 
    * We will be <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate">concatinating</a> the both inputs on the last axis.
    * After the concatination we will be applying <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D"> Average pooling </a> which reduces the size of feature map by $N\times$ times
    * In the paper it was mentioned that to apply a group convolutions, but for the assignment we will be applying the simple conv layers with kernel size $(3*3)$
    * We are skipping the channel shuffling 
    * similarly we will be applying a simple conv layers with kernel size $(3*3)$ consider this output is X
    * later we will get the Y=(X $\otimes \sigma((1\times1)conv(relu((1\times1)conv(X))))) \oplus X$, here $\oplus$ is elementwise addition and $\otimes$ is elementwise multiplication
    * Finally apply <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> / <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)
    * If you use <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> then use bilinear pooling as interpolation technique

NOTE: here N times reduction and N time increments makes the input and out shape same, you can explore with the N values, you can choose N = 2 or 4

* Example with N=2:
    * Assume the C4 is of shape (64,64,64) then the shape of GF will be (64,64,32)
    * Assume the C4 is of shape (64,64,64) and the shape of GF is (64,64,32) then the shape of CF1 will be (64,64,32)
    * Assume the C4 is of shape (64,64,64) and the shape of CF1 is (64,64,32) then the shape of CF2 will be (64,64,32)
    * Assume the C4 is of shape (64,64,64) and the shape of CF2 is (64,64,32) then the shape of CF3 will be (64,64,32)
"""

class global_flow(tf.keras.layers.Layer):
    def __init__(self, name="global_flow"):
        super().__init__(name=name)
        
    def call(self, X):
        # implement the global flow operatiom
        return X

class context_flow(tf.keras.layers.Layer):    
    def __init__(self, name="context_flow"):
        super().__init__(name=name)
    def call(self, X):
        # here X will a list of two elements 
        INP, FLOW = X[0], X[1] 
        # implement the context flow as mentioned in the above cell
        return X

"""* As shown in the above architecture we will be having 4 context flows
* if you have implemented correctly all the shapes of Global Flow, and 3 context flows will have the same dimension
* the output of these 4 modules will be <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add">added</a> to get the same output matrix
<img src='https://i.imgur.com/Bu63AAA.png' width="400">
 * The output of after the sum, will be sent to the <strong>Feature selection module $FSM$</strong>
 
* Example:
    * if the shapes of GF, CF1, CF2, CF3 are (64,64,32), (64,64,32), (64,64,32), (64,64,32), (64,64,32) respectivly then after the sum we will be getting (64,64,32), which will be passed to the next module.
 
<strong>Feature selection module</strong>:

* As part of the FSM we will be applying a conv layer (3,3) with the padding="same" so that the output and input will have same shapes
* Let call the output as X
* Pass the X to global pooling which results the matrix (#, 1, 1, number_of_channels)
* Apply $1*1$ conv layer, after the pooling
* the output of the $1*1$ conv layer will be passed to the Batch normalization layer, followed by Sigmoid activation function.
* we will be having the output matrix of shape (#, 1, 1, number_of_channels) lets call it 'Y'
* <strong>we can interpret this as attention mechanisum, i.e for each channel we will having a weight</strong>
* the dimension of X (#, w, h, k) and output above steps Y is (#, 1, 1, k) i.e we need to multiply each channel of X will be <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Multiply">multiplied</a> with corresponding channel of Y
* After creating the weighted channel map we will be doing upsampling such that it will double the height and width.
* apply <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> with bilinear pooling as interpolation technique

* <font color="red">Example</font>:
    * Assume the matrix shape of the input is (64,64,32) then after upsampling it will be (128,128,32)
"""

class fsm(tf.keras.layers.Layer):    
    def __init__(self, name="feature_selection"):
        super().__init__(name=name)
        
    def call(self, X):
        # implement the FSM modules based on image in the above cells
        return FSM_Conv_T

"""* <b>Adapted Global Convolutional Network (AGCN)</b>:
    <img src="https://i.imgur.com/QNB8RmV.png" width="300">
    
    * AGCN will get the input from the output of the "conv block" of $C_1$
    
    * In all the above layers we will be using the padding="same" and stride=(1,1)
    
    * so that we can have the input and output matrices of same size
    
* <font color="red">Example</font>:
    * Assume the matrix shape of the input is (128,128,32) then the output it will be (128,128,32)
"""

class agcn(tf.keras.layers.Layer):    
    def __init__(self, name="global_conv_net"):
        super().__init__(name=name)
        
    def call(self, X):
        # please implement the above mentioned architecture
        return X

"""*     <img src='https://i.imgur.com/prH3Mno.png' width="600">
* as shown in the architecture, after we get the AGCN it will get concatinated with the FSM output

* If we observe the shapes both AGCN and FSM will have same height and weight

* we will be concatinating both these outputs over the last axis

* The concatinated output will be passed to a conv layers with filters = number of classes in our data set and the activation function = 'relu'

* we will be using padding="same" which results in the same size feature map

* If you observe the shape of matrix, it will be 4x times less than the original image

* to make it equal to the original output shape, we will do 4x times upsampling of rows and columns

* apply <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D">upsampling</a> with bilinear pooling as interpolation technique

* Finally we will be applying sigmoid activation.

* Example:
    * Assume the matrix shape of AGCN is (128,128,32)  and FSM is (128,128,32) the concatination will make it (128, 128, 64)
    * Applying conv layer will make it (128,128,21)
    * Finally applying upsampling will make it (512, 512, 21)
    * Applying sigmoid will result in the same matrix (512, 512, 21)
"""

X_input = Input(shape=(128,128,3))

# Stage 1
X = Conv2D(64, (3, 3), name='conv1', padding="same", kernel_initializer=glorot_uniform(seed=0))(X_input)
X = BatchNormalization(axis=3, name='bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((2, 2), strides=(2, 2))(X)
print(X.shape)

"""* If you observe the arcitecture we are creating a feature map with 2x time less width and height
* we have written the first stage of the code above.
* Write the next layers by using the custom layers we have written
"""

# write the complete architecutre

model = Model(inputs = X, outputs = output)

model.summary()

tf.keras.utils.plot_model(
    model, to_file='model4.png', show_shapes=True, show_layer_names=True,
    rankdir='TB')

"""### Usefull tips:
* use "interpolation=cv2.INTER_NEAREST" when you are resizing the image, so that it won't mess with the number of classes
* keep the images in the square shape like $256*256$ or $512*512$
* Carefull when you are converting the (W, H) output image into (W, H, Classes)
* Even for the canet, use the segmentation model's losses and the metrics
* The goal of this assignment is make you familier in with computer vision problems, image preprocessing, building complex architectures and implementing research papers, so that in future you will be very confident in industry
* you can use the tensorboard logss to see how is yours model's training happening
* use callbacks that you have implemented in previous assignments

### Things to keep in mind

* You need to train  above built model and plot the train and test losses.
* Make sure there is no overfitting, you are free play with the identity blocks in C1, C2, C3, C4
* before we apply the final sigmoid activation, you can add more conv layers or BN or dropouts etc
* you are free to use any other optimizer or learning rate or weights init or regularizations
"""
